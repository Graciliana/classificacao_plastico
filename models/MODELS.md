# ‚ôªÔ∏è Classifica√ß√£o de Pl√°sticos com Vis√£o Computacional

Este projeto realiza a classifica√ß√£o de imagens de pl√°sticos em 7 categorias usando diferentes algoritmos de machine learning aplicados a vetores extra√≠dos das imagens em tons de cinza redimensionadas (`64x64`).

## üîç Objetivo

Avaliar e comparar o desempenho dos seguintes algoritmos:

* **Logistic Regression**
* **Naive Bayes**
* **Random Forest**
* **SVM (Support Vector Machine)**

---

## üì¶ Dataset

O conjunto de dados cont√©m imagens organizadas em pastas por classe: `PET`, `PP`, `PS`, `PVC`, `HDPE`, `LDPA`, `Other`, divididas em treino, valida√ß√£o e teste.

---

## ü§ñ Modelos Utilizados

### 1. Logistic Regression

**Como funciona:**
Modelo estat√≠stico que calcula a probabilidade de uma amostra pertencer a uma classe usando uma fun√ß√£o log√≠stica (sigmoide). Para m√∫ltiplas classes, usamos a vers√£o **multinomial softmax**.

**Uso na classifica√ß√£o de imagens:**
As imagens s√£o transformadas em vetores (flatten), normalizadas e, opcionalmente, reduzidas com PCA. A regress√£o log√≠stica tenta encontrar um hiperplano que separe as classes.

**Vantagens:**

* Simples e interpret√°vel
* R√°pido para treinar
* Funciona bem com dados lineares

**Desvantagens:**

* Desempenho limitado com dados complexos
* Pouco robusto a ru√≠do visual e sobreposi√ß√£o de classes

---

### 2. Naive Bayes

**Como funciona:**
Baseado no Teorema de Bayes, assume independ√™ncia entre as vari√°veis de entrada (pixels). Estima a probabilidade de uma imagem pertencer a cada classe.

**Uso na classifica√ß√£o de imagens:**
Ap√≥s transforma√ß√£o e normaliza√ß√£o dos pixels, o modelo calcula as probabilidades de ocorr√™ncia de cada valor para cada classe.

**Vantagens:**

* Extremamente r√°pido
* Funciona bem mesmo com poucos dados
* F√°cil de implementar

**Desvantagens:**

* Assun√ß√£o de independ√™ncia dos pixels raramente √© verdadeira
* Baixo desempenho quando os atributos s√£o correlacionados (como em imagens)

---

### 3. Random Forest

**Como funciona:**
Conjunto (ensemble) de √°rvores de decis√£o que votam para decidir a classe final. Cada √°rvore √© treinada com um subconjunto aleat√≥rio de dados e atributos.

**Uso na classifica√ß√£o de imagens:**
Ap√≥s vetorizar as imagens, cada √°rvore tenta capturar padr√µes locais dos pixels. A m√©dia dos votos das √°rvores define a classe final.

**Vantagens:**

* Robusto ao overfitting
* Lida bem com dados n√£o lineares
* Funciona com vari√°veis ruidosas

**Desvantagens:**

* Pode ser lento em infer√™ncia
* Modelo grande, dif√≠cil de interpretar

---

### 4. SVM (Support Vector Machine)

**Como funciona:**
Busca um hiperplano que maximize a margem entre as classes. Com o kernel RBF, consegue separar classes com limites n√£o lineares.

**Uso na classifica√ß√£o de imagens:**
Ap√≥s a redu√ß√£o da dimensionalidade (PCA), o SVM busca uma fronteira √≥tima entre as classes em um espa√ßo de menor dimens√£o.

**Vantagens:**

* Excelente desempenho em datasets com margens claras entre as classes
* Robusto a outliers
* Muito eficaz em espa√ßos com muitas dimens√µes

**Desvantagens:**

* Treinamento mais lento
* Pouco escal√°vel para grandes volumes de dados

---

## üìä M√©tricas Avaliadas

* **Acur√°cia**
* **F1-score ponderado**
* **Log Loss**
* **Tempo de treinamento**
* **Tempo de infer√™ncia por imagem**
* **Matriz de confus√£o**
* **Visualiza√ß√£o das previs√µes**

---

## üìÅ Estrutura dos Notebooks

* `01_logistic_regression.ipynb`
* `02_naive_bayes.ipynb`
* `03_random_forest.ipynb`
* `04_svm.ipynb`
* `comparacao_modelos.ipynb`

---

## üöÄ Pr√≥ximos passos

* Explorar redes neurais (CNNs)
* Avaliar com imagens RGB
* Aplicar t√©cnicas de aumento de dados (Data Augmentation)
* Implementar interface com Streamlit para demonstra√ß√£o interativa
